{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling2D\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# This file has 25000 samples per class and 2 classes (Signal, Background)\n",
    "\n",
    "dataset = \"ZTraining_4momenta-M.csv\"\n",
    "categories = [\"Background\", \"Signal\"]\n",
    "samples_class = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data from database file...\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "print(\"\\n Loading data from database file...\")\n",
    "features = np.loadtxt(dataset, delimiter = ',', skiprows=1,\n",
    "                            usecols = (0,8), unpack=True) #.astype(np.float32)\n",
    "labels = np.loadtxt(dataset, delimiter = ',', skiprows=1, dtype = 'str', \n",
    "                            usecols = (9), unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = np.array(labels)\n",
    "features = features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generating training and test data...\n",
      "\n",
      " Total samples:    50000 \n",
      "\n",
      " Training samples: 40000 \n",
      "\n",
      " Test samples:     10000 \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Generating training and test data...\")\n",
    "x_train = np.concatenate((\n",
    "            features[:(samples_class-5000)],\n",
    "            features[samples_class:(2*samples_class-5000)]),\n",
    "            axis=0)\n",
    "x_test = np.concatenate((\n",
    "            features[(samples_class-5000):samples_class],\n",
    "            features[(2*samples_class-5000):2*samples_class]),\n",
    "            axis=0)\n",
    "\n",
    "y_train = np.concatenate((\n",
    "            class_num[:(samples_class-5000)],\n",
    "            class_num[samples_class:(2*samples_class-5000)]),\n",
    "            axis=0)\n",
    "y_test = np.concatenate((\n",
    "            class_num[(samples_class-5000):samples_class],\n",
    "            class_num[(2*samples_class-5000):2*samples_class]),\n",
    "            axis=0)\n",
    "\n",
    "print(\"\\n Total samples:    {} \".format(len(labels)))\n",
    "print(\"\\n Training samples: {} \".format(len(y_train)))\n",
    "print(\"\\n Test samples:     {} \".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "Dataset = pd.read_csv('ZTraining_4momenta-M.csv')\n",
    "sns_plot = sns.pairplot(Dataset, hue = 'Class', palette=\"husl\", vars=['E1','px1','py1','pz1',\n",
    "                                                                      'E2','px2','py2','pz2','M'])\n",
    "sns_plot.savefig(\"4-Momenta-M.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "#======================== Model Building =======================================\n",
    "#===============================================================================\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "epochs = 10\n",
    "num_features = 1\n",
    "num_classes = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building model...\n",
      "\n",
      " Compiling model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 17,154\n",
      "Trainable params: 17,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " Fitting model...\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 2s 57us/sample - loss: 0.6934 - acc: 0.5041 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.6921 - acc: 0.5111 - val_loss: 0.6912 - val_acc: 0.5177\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.6920 - acc: 0.5139 - val_loss: 0.6925 - val_acc: 0.5026\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.6917 - acc: 0.5127 - val_loss: 0.6909 - val_acc: 0.5185\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.6917 - acc: 0.5147 - val_loss: 0.6909 - val_acc: 0.5175\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.6917 - acc: 0.5131 - val_loss: 0.6913 - val_acc: 0.5103\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 2s 47us/sample - loss: 0.6918 - acc: 0.5140 - val_loss: 0.6910 - val_acc: 0.5174\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 2s 47us/sample - loss: 0.6916 - acc: 0.5140 - val_loss: 0.6909 - val_acc: 0.5172\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.6914 - acc: 0.5146 - val_loss: 0.6921 - val_acc: 0.5146\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 2s 52us/sample - loss: 0.6914 - acc: 0.5155 - val_loss: 0.6911 - val_acc: 0.5110\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "print(\"\\n Building model...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes,activation=tf.nn.softmax))\n",
    "\n",
    "print(\"\\n Compiling model...\")\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print(\"\\n Fitting model...\")\n",
    "history = model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(history.epoch,history.history['loss'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_loss'], label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(history.epoch,history.history['acc'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_acc'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "Dataset = pd.read_csv('ZTraining_4momenta-M.csv')\n",
    "sns_plot = sns.pairplot(Dataset, hue = 'Class', palette=\"husl\", vars=['M'])\n",
    "#sns_plot.savefig(\"4-Momenta-M.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(history.epoch,history.history['acc'], label='training')\n",
    "plt.plot(history.epoch,history.history['val_acc'], label='validation')\n",
    "plt.title('accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
